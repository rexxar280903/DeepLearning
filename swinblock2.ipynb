{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435f9d09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:01:09.588784Z",
     "iopub.status.busy": "2025-11-12T07:01:09.588512Z",
     "iopub.status.idle": "2025-11-12T07:02:36.565663Z",
     "shell.execute_reply": "2025-11-12T07:02:36.564908Z"
    },
    "papermill": {
     "duration": 86.984662,
     "end_time": "2025-11-12T07:02:36.567287",
     "exception": false,
     "start_time": "2025-11-12T07:01:09.582625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet lpips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3215207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:02:36.643002Z",
     "iopub.status.busy": "2025-11-12T07:02:36.642741Z",
     "iopub.status.idle": "2025-11-12T07:02:50.232649Z",
     "shell.execute_reply": "2025-11-12T07:02:50.232021Z"
    },
    "papermill": {
     "duration": 13.629032,
     "end_time": "2025-11-12T07:02:50.233944",
     "exception": false,
     "start_time": "2025-11-12T07:02:36.604912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "from tqdm import tqdm # Opsional, tapi sangat disarankan untuk progress bar\n",
    "from torch.utils.data import random_split\n",
    "from math import floor\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- import LPIPS ---\n",
    "try:\n",
    "    import lpips  # pip install lpips\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Package 'lpips' belum terpasang. Jalankan: pip install lpips\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d0db94",
   "metadata": {
    "papermill": {
     "duration": 0.036324,
     "end_time": "2025-11-12T07:02:50.307495",
     "exception": false,
     "start_time": "2025-11-12T07:02:50.271171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fungsi train, custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967a875b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:02:50.380933Z",
     "iopub.status.busy": "2025-11-12T07:02:50.380559Z",
     "iopub.status.idle": "2025-11-12T07:02:51.709905Z",
     "shell.execute_reply": "2025-11-12T07:02:51.709305Z"
    },
    "papermill": {
     "duration": 1.367946,
     "end_time": "2025-11-12T07:02:51.711310",
     "exception": false,
     "start_time": "2025-11-12T07:02:50.343364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, csv, time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "\n",
    "# --- import LPIPS ---\n",
    "try:\n",
    "    import lpips  # pip install lpips\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Package 'lpips' belum terpasang. Jalankan: pip install lpips\") from e\n",
    "\n",
    "def _read_last_epoch(csv_path):\n",
    "    if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if 'epoch' in df.columns and pd.api.types.is_numeric_dtype(df['epoch']):\n",
    "            df_epoch = df['epoch'].dropna()\n",
    "            return int(df_epoch.max()) if len(df_epoch) else 0\n",
    "        return 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs,\n",
    "    save_dir=\"model_checkpoints\", path=\"/kaggle/working/histor.csv\",\n",
    "    continue_global_epoch=True\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # --- metrics ---\n",
    "    psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "    # LPIPS pakai backbone VGG; output: \"semakin kecil semakin baik\"\n",
    "    lpips_fn = lpips.LPIPS(net='vgg').to(device)\n",
    "    lpips_fn.eval()\n",
    "\n",
    "    # helper untuk map [0,1] -> [-1,1] (LPIPS requirement)\n",
    "    def _to_minus1_1(x): \n",
    "        return x * 2.0 - 1.0\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [], 'val_psnr': [], 'val_ssim': [], 'val_lpips': []\n",
    "    }\n",
    "\n",
    "    run_id = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    start_epoch_global = _read_last_epoch(path) if continue_global_epoch else 0\n",
    "    print(f\"Mulai Training di device: {device} | run_id={run_id} | start_epoch_global={start_epoch_global}\")\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        epoch_in_run = e + 1\n",
    "        epoch_global = start_epoch_global + epoch_in_run\n",
    "\n",
    "        # ===== TRAIN =====\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch_in_run}/{num_epochs} [Train]\")\n",
    "        for lr_images, hr_images in train_pbar:\n",
    "            lr_images = lr_images.to(device); hr_images = hr_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(lr_images)\n",
    "            loss = criterion(pred, hr_images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * lr_images.size(0)\n",
    "            train_pbar.set_postfix({'train_loss': loss.item()})\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "\n",
    "        # ===== VAL =====\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        psnr_metric.reset(); ssim_metric.reset()\n",
    "        lpips_sum = 0.0\n",
    "\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch_in_run}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for lr_images, hr_images in val_pbar:\n",
    "                lr_images = lr_images.to(device); hr_images = hr_images.to(device)\n",
    "                pred = model(lr_images)\n",
    "\n",
    "                # loss biasa\n",
    "                vloss = criterion(pred, hr_images)\n",
    "                running_val_loss += vloss.item() * lr_images.size(0)\n",
    "\n",
    "                # PSNR & SSIM (input [0,1])\n",
    "                psnr_metric.update(pred, hr_images)\n",
    "                ssim_metric.update(pred, hr_images)\n",
    "\n",
    "                # LPIPS (harus [-1,1])\n",
    "                lpips_batch = lpips_fn(_to_minus1_1(pred), _to_minus1_1(hr_images))\n",
    "                # lpips_fn return shape (N,1,1,1) atau (N,) tergantung versi → ambil rata-rata\n",
    "                lpips_mean = lpips_batch.mean().item()\n",
    "                lpips_sum += lpips_mean * lr_images.size(0)\n",
    "\n",
    "                val_pbar.set_postfix({'val_loss': vloss.item(), 'lpips': lpips_mean})\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_psnr = psnr_metric.compute().item()\n",
    "        epoch_ssim = ssim_metric.compute().item()\n",
    "        epoch_lpips = lpips_sum / len(val_loader.dataset)\n",
    "\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_psnr'].append(epoch_psnr)\n",
    "        history['val_ssim'].append(epoch_ssim)\n",
    "        history['val_lpips'].append(epoch_lpips)\n",
    "\n",
    "        print(f\"[RESULT] e{epoch_in_run} | val_loss={epoch_val_loss:.6f} | PSNR={epoch_psnr:.4f} | SSIM={epoch_ssim:.4f} | LPIPS={epoch_lpips:.4f}\")\n",
    "\n",
    "        # ===== SAVE CHECKPOINT =====\n",
    "        ckpt_name = f\"model_{run_id}_e{epoch_in_run}.pth\"\n",
    "        save_path = os.path.join(save_dir, ckpt_name)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"[SAVE] {save_path}\")\n",
    "\n",
    "        # ===== APPEND CSV (tambahkan kolom val_lpips) =====\n",
    "        is_new = not os.path.exists(path)\n",
    "        with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.DictWriter(\n",
    "                f,\n",
    "                fieldnames=[\"epoch\",\"epoch_in_run\",\"run_id\",\n",
    "                            \"train_loss\",\"val_loss\",\"val_psnr\",\"val_ssim\",\"val_lpips\",\"nama_model\"]\n",
    "            )\n",
    "            if is_new or os.path.getsize(path) == 0:\n",
    "                w.writeheader()\n",
    "            w.writerow({\n",
    "                \"epoch\":        epoch_global,\n",
    "                \"epoch_in_run\": epoch_in_run,\n",
    "                \"run_id\":       run_id,\n",
    "                \"train_loss\":   float(epoch_train_loss),\n",
    "                \"val_loss\":     float(epoch_val_loss),\n",
    "                \"val_psnr\":     float(epoch_psnr),\n",
    "                \"val_ssim\":     float(epoch_ssim),\n",
    "                \"val_lpips\":    float(epoch_lpips),   # ⬅️ baru\n",
    "                \"nama_model\":   ckpt_name\n",
    "            })\n",
    "\n",
    "    print(\"Training Selesai.\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6879fe21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:02:51.803455Z",
     "iopub.status.busy": "2025-11-12T07:02:51.802889Z",
     "iopub.status.idle": "2025-11-12T07:02:51.811293Z",
     "shell.execute_reply": "2025-11-12T07:02:51.810369Z"
    },
    "papermill": {
     "duration": 0.058385,
     "end_time": "2025-11-12T07:02:51.812546",
     "exception": false,
     "start_time": "2025-11-12T07:02:51.754161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SuperResolutionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset kustom untuk super-resolution.\n",
    "    Akan memuat gambar 256x256 (HR) dan secara otomatis\n",
    "    membuat versi 128x128 (LR) sebagai input.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_folder):\n",
    "        self.root_dir = image_folder\n",
    "        # Dapatkan daftar semua nama file gambar di folder\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        # Definisikan transform untuk HR (hanya konversi ke Tensor)\n",
    "        self.hr_transform = T.ToTensor()\n",
    "        \n",
    "        # Definisikan transform untuk LR (Resize ke 128x128 + konversi ke Tensor)\n",
    "        self.lr_transform = T.Compose([\n",
    "            T.Resize((128, 128), interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        # Mengembalikan jumlah total gambar\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Dapatkan path lengkap ke satu gambar\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        \n",
    "        # 2. Buka gambar HR (256x256) menggunakan PIL\n",
    "        #    Gunakan .convert('RGB') untuk memastikan gambar punya 3 channel\n",
    "        hr_image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 3. Buat tensor HR dan LR dari gambar yang sama\n",
    "        lr_tensor = self.lr_transform(hr_image)\n",
    "        hr_tensor = self.hr_transform(hr_image)\n",
    "        \n",
    "        # 4. Kembalikan pasangan (input, target)\n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fa9f05",
   "metadata": {
    "papermill": {
     "duration": 0.039054,
     "end_time": "2025-11-12T07:02:51.888912",
     "exception": false,
     "start_time": "2025-11-12T07:02:51.849858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ARSITEKTUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041574da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:02:51.964732Z",
     "iopub.status.busy": "2025-11-12T07:02:51.964479Z",
     "iopub.status.idle": "2025-11-12T07:02:51.987465Z",
     "shell.execute_reply": "2025-11-12T07:02:51.986698Z"
    },
    "papermill": {
     "duration": 0.062894,
     "end_time": "2025-11-12T07:02:51.988546",
     "exception": false,
     "start_time": "2025-11-12T07:02:51.925652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class blocks (nn.Module):\n",
    "\n",
    "    def __init__(self, k_feature, N,H,W,C,j, head):\n",
    "        super().__init__()\n",
    "        self.j = j\n",
    "        self.N, self.C, self.H, self.W = N,C,H,W\n",
    "        self.k_feature = k_feature\n",
    "        self.head = head\n",
    "        self.dk = self.k_feature//self.head # dmodel = k_feature\n",
    "        self.n_win = self.H//self.j * self.W//self.j\n",
    "        self.rasio_ekspansi = 4\n",
    "    \n",
    "    # ============================ body Rir =====================================\n",
    "        # normalisasi\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=[self.k_feature])\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=[self.k_feature])\n",
    "\n",
    "        # Linear projections\n",
    "        self.wq = nn.Parameter(torch.empty(self.k_feature, self.k_feature))\n",
    "        self.wk = nn.Parameter(torch.empty(self.k_feature, self.k_feature))\n",
    "        self.wv = nn.Parameter(torch.empty(self.k_feature, self.k_feature))\n",
    "        self.wo = nn.Parameter(torch.empty(self.k_feature, self.k_feature))\n",
    "        self.bt = nn.Parameter(torch.zeros(self.head, self.j*self.j, self.j*self.j))\n",
    "\n",
    "        # MLP\n",
    "        self.w1 = nn.Parameter(torch.empty(self.k_feature, self.k_feature * self.rasio_ekspansi))\n",
    "        self.b1 = nn.Parameter(torch.zeros([self.k_feature*self.rasio_ekspansi]))\n",
    "        self.w2 = nn.Parameter(torch.empty(self.k_feature*self.rasio_ekspansi, self.k_feature))\n",
    "        self.b2 = nn.Parameter(torch.zeros([self.k_feature]))\n",
    "\n",
    "        # Panggil fungsi inisialisasi\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # === Xavier initialization untuk layer linear ===\n",
    "        nn.init.xavier_uniform_(self.wq)\n",
    "        nn.init.xavier_uniform_(self.wk)\n",
    "        nn.init.xavier_uniform_(self.wv)\n",
    "        nn.init.xavier_uniform_(self.wo)\n",
    "        nn.init.xavier_uniform_(self.w1)\n",
    "        nn.init.xavier_uniform_(self.w2)\n",
    "\n",
    "        # Bias nol sudah oke (default)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.zeros_(self.b2)\n",
    "\n",
    "        # Bisa juga He init jika ingin lebih cocok untuk ReLU:\n",
    "        # nn.init.kaiming_normal_(self.w1, nonlinearity='relu')\n",
    "        # nn.init.kaiming_normal_(self.w2, nonlinearity='relu')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \n",
    "        # input kedala fungsi ini x [8,96,128,128]\n",
    "        \n",
    "        # =================================================--=======================================================\n",
    "        # ============================================== MSA =======================================================\n",
    "        # =================================================--=======================================================\n",
    "        \n",
    "        # menukra ukuran dimensi (N,C,H,W) => (N,H,W,C)\n",
    "        xp = x.permute(0,2,3,1)\n",
    "        \n",
    "        # normalisasi input\n",
    "       \n",
    "        xln1 = self.layernorm1(xp)\n",
    "        N_dyn, H_dyn, W_dyn, C_dyn = xln1.shape\n",
    "\n",
    "        # window partition. \n",
    "        # reshape (N,H,W,C) => ((N X n_win),j,j,k_feature)\n",
    "        # 1.view potong H dan W menjadi (numwin_H, j) dan (NumWin_W, j)\n",
    "        xr = xln1.view(N_dyn, H_dyn//self.j, self.j, W_dyn//self.j, self.j, C_dyn)\n",
    "        \n",
    "        # 2. permute [N, H//J, W//J, j,j, k_feature]\n",
    "        xwin = xr.permute(0,1,3,2,4,5).contiguous()\n",
    "\n",
    "        # 3.view gabugnkan semua jendela kedalam dimenci batch\n",
    "        # [N * num_wind * numwind , j,j, C]\n",
    "        xwin = xwin.view(-1,self.j,self.j,C_dyn)\n",
    "        # xr = xln1.reshape(self.N * self.n_win, self.j, self.j, self.k_feature)\n",
    "\n",
    "        # flatten ((N X n_win),j,j,C) => ((N X n_win),j*j,C)  \n",
    "        xf = xwin.reshape(-1, self.j * self.j, self.k_feature)\n",
    "\n",
    "        # self attention\n",
    "        # mencari Q,K,V\n",
    "        Q = xf @ self.wq\n",
    "        K = xf @ self.wk\n",
    "        V = xf @ self.wv\n",
    "\n",
    "        # split heads mengubah ukuran Q,K,V [2048,64,96] => menjadi [2048,64,6,16]\n",
    "        Q = Q.reshape((-1, self.j * self.j, self.head, self.k_feature//self.head))\n",
    "        K = K.reshape((-1, self.j * self.j, self.head, self.k_feature//self.head))\n",
    "        V = V.reshape((-1, self.j * self.j, self.head, self.k_feature//self.head))\n",
    "\n",
    "        # permute dari bentuk [2048,64,6,16] => [2048,6,64,16]\n",
    "        Q = Q.permute(0,2,1,3)\n",
    "        K = K.permute(0,2,1,3)\n",
    "        V = V.permute(0,2,1,3)\n",
    "\n",
    "        # SCALED dot product attention\n",
    "        score = Q.matmul(K.transpose(-2,-1))\n",
    "        scaling = score/(self.dk ** 0.5)\n",
    "        B = scaling + self.bt\n",
    "        P = F.softmax(B, dim=-1)\n",
    "        O = P @ V\n",
    "\n",
    "        #permute dari [2048,6,64,16] => [2048,64,6,16]\n",
    "        O = O.permute(0,2,1,3)\n",
    "        \n",
    "        #reshape menggabungkan head dari [2048,64,6,16] => [2048,64,96]\n",
    "        O = O.reshape(-1, self.j * self.j, self.k_feature )\n",
    "\n",
    "        # MSA_out\n",
    "        MSA_out = O @ self.wo\n",
    "\n",
    "        # mengubah kembali ke bentuk input awal [N,C,H,W]\n",
    "        # 1. Reshape ke [256, 8, 8, 96]\n",
    "        MSA_out = MSA_out.view(-1, self.j, self.j, self.k_feature) \n",
    "        \n",
    "        # 2. Reshape ke [N_dyn, NumWin_H, NumWin_W, j, j, C]\n",
    "        MSA_out = MSA_out.view(N_dyn, H_dyn // self.j, W_dyn // self.j, self.j, self.j, self.k_feature)\n",
    "        \n",
    "        # 3. Permute\n",
    "        MSA_out = MSA_out.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        \n",
    "        # 4. View ke [N_dyn, H_dyn, W_dyn, C]\n",
    "        MSA_out = MSA_out.view(N_dyn, H_dyn, W_dyn, self.k_feature)\n",
    "\n",
    "        # # reshape dari ukuran [2048,64,96] => [2048,8,8,96]\n",
    "        # MSA_out = MSA_out.reshape(self.N * self.n_win,self.j, self.j, self.k_feature)\n",
    "\n",
    "        # # reshape dari ukuran [2048,8,8,96] => [8,256,8,8,96]\n",
    "        # MSA_out = MSA_out.reshape(self.N, self.n_win, self.j, self.j, self.k_feature)\n",
    "\n",
    "        # # reshape dari ukuran [8,256,8,8,96] => [8,128,128,96]\n",
    "        # MSA_out = MSA_out.reshape(self.N, self.H, self.W, self.k_feature)\n",
    "\n",
    "        # permute untuk mengembalikan ke bentuk awal N,C,H,W\n",
    "        MSA_out = MSA_out.permute(0,-1,1,2)\n",
    "\n",
    "        #============ RESIDUAL =======================\n",
    "        xmsa = x + MSA_out # [N,k_feature, H,W] [8.96,128,128]\n",
    "\n",
    "        # =================================================--=======================================================\n",
    "        # ============================================== MLP =======================================================\n",
    "        # =================================================--=======================================================\n",
    "\n",
    "        # input x [N,k_feature, H,W] [8.96,128,128]\n",
    "        x2 = xmsa.clone()\n",
    "       # menukra ukuran dimensi (N,C,H,W) => (N,H,W,C)\n",
    "        xp = x2.permute(0,2,3,1)\n",
    "    \n",
    "        # normalisasi input\n",
    "        xln = self.layernorm2(xp)\n",
    "\n",
    "        # Multi Layer Perceptron \n",
    "        # layer1 (8,128,128,96) => (8,128,128,384)\n",
    "        x2 = xln @ self.w1 + self.b1\n",
    "        # aktivasi\n",
    "        x2 = F.gelu(x2)\n",
    "\n",
    "        # layer2 (8,128,128,384) => (8,128,128,96)\n",
    "        x2 = x2 @ self.w2 + self.b2\n",
    "\n",
    "        # permutasi\n",
    "        x2 = x2.permute(0,3,1,2)\n",
    "\n",
    "        # residual\n",
    "        x_out = x2 + xmsa\n",
    "        return x_out\n",
    "\n",
    "class group(nn.Module):\n",
    "    def __init__(self, k_feature, N,H,W,C,j, head, j_blocks):\n",
    "        super().__init__()\n",
    "        self.j = j\n",
    "        self.N, self.C, self.H, self.W = N,C,H,W\n",
    "        self.k_feature = k_feature\n",
    "        self.head = head\n",
    "        self.dk = self.k_feature//self.head # dmodel = k_feature\n",
    "        self.n_win = self.H//self.j * self.W/self.j\n",
    "        \n",
    "        self.bloks = nn.ModuleList([\n",
    "            blocks(k_feature=self.k_feature, N=self.N,H=self.H,W=self.W,C=self.C,j=self.j, head = self.head) for _ in range(j_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        xin = x.clone()\n",
    "        for blk in self.bloks:\n",
    "            x = blk(x)\n",
    "        # residual \\\n",
    "        x_out = xin + x\n",
    "        return x_out\n",
    "\n",
    "    \n",
    "\n",
    "class RIR_interpolation(nn.Module):\n",
    "\n",
    "    def __init__(self, N,C,H,W,  k_feature, j_group, j_blocks):\n",
    "        super().__init__()\n",
    "        self.N, self.C, self.H, self.W = N,C,H,W\n",
    "        # self.groups = groups\n",
    "        self.blocks = j_blocks\n",
    "        self.j = 8  # ukuran window\n",
    "        self.k_feature = k_feature\n",
    "        self.head = 6\n",
    "        self.dk = self.k_feature//self.head # dmodel = k_feature\n",
    "        self.j_group = j_group\n",
    "        \n",
    "        self.n_win = self.H//self.j * self.W//self.j\n",
    "        \n",
    "        # [out_channels, in_channels, kernel, kernel]\n",
    "\n",
    "        # ========================== ekstraksi fitur ================================\n",
    "        self.w0 = nn.Parameter(torch.empty(self.k_feature, 3, 3, 3))\n",
    "        self.w1 = nn.Parameter(torch.empty(self.k_feature * 4, k_feature, 3, 3))\n",
    "        self.w2 = nn.Parameter(torch.empty(3, self.k_feature, 3, 3))\n",
    "        self.b0 = nn.Parameter(torch.zeros(self.k_feature))\n",
    "        self.b1 = nn.Parameter(torch.zeros(self.k_feature * 4))\n",
    "        self.b2 = nn.Parameter(torch.zeros(3))\n",
    "\n",
    "\n",
    "        # body \n",
    "        self.body = nn.ModuleList([\n",
    "            group(self.k_feature, self.N, self.H, self.W, self.C, self.j, self.head, 6) for _ in range(self.j_group)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.pixshuffle = nn.PixelShuffle(upscale_factor=2)\n",
    "        self.init_weights()\n",
    "    \n",
    "\n",
    "    def init_weights(self):\n",
    "        # He initialization untuk convolution\n",
    "        nn.init.kaiming_normal_(self.w0, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.w1, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.w2, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.b0)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.zeros_(self.b2)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # =================Ekstraksi Fitur===========================\n",
    "        # input awal X [N,C,H,W] [8,3,128,128]\n",
    "        d = F.conv2d(x, self.w0, bias=self.b0, padding=1)\n",
    "        d = F.relu(d) # [8,96,128,128]\n",
    "        # output = X = [N,k_feature,H,W] [8,96,128,128]\n",
    "\n",
    "        \n",
    "        # ================== body rir ==============================\n",
    "        #  input awal X [N,k_featur,H,W] = [8,96,128,128]\n",
    "        \n",
    "        # b = self.body.forward(d)\n",
    "        din = d.clone()\n",
    "        for grp in self.body:\n",
    "            d = grp(d)\n",
    "\n",
    "        # residual\n",
    "        b = din + d\n",
    "            \n",
    "\n",
    "        # output sama dengan input X = X [N,k_featur,H,W] = [8,96,128,128]\n",
    "\n",
    "        \n",
    "        # =============== Upsampler/pixxel shuffle ==================\n",
    "        # input awal X [N,k_featur,H,W] = [8,96,128,128]\n",
    "        d = F.conv2d(b, self.w1, bias=self.b1, padding=1)\n",
    "        d = F.relu(d)\n",
    "        o = self.pixshuffle(d)\n",
    "\n",
    "        # Rekonstruksi\n",
    "        xsr = F.conv2d(o, self.w2, bias=self.b2, padding=1)\n",
    "        # output X [N.C,H*2,W*2]  [8,3,256,256]\n",
    "        \n",
    "        return xsr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240c79a",
   "metadata": {
    "papermill": {
     "duration": 0.035981,
     "end_time": "2025-11-12T07:02:52.061048",
     "exception": false,
     "start_time": "2025-11-12T07:02:52.025067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f74aa20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:02:52.134978Z",
     "iopub.status.busy": "2025-11-12T07:02:52.134295Z",
     "iopub.status.idle": "2025-11-12T07:02:52.139298Z",
     "shell.execute_reply": "2025-11-12T07:02:52.138728Z"
    },
    "papermill": {
     "duration": 0.043049,
     "end_time": "2025-11-12T07:02:52.140369",
     "exception": false,
     "start_time": "2025-11-12T07:02:52.097320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_architecture, path_hist):\n",
    "    \"\"\"\n",
    "    Memuat bobot (weights) yang tersimpan ke dalam arsitektur model.\n",
    "\n",
    "    Args:\n",
    "        model_architecture (nn.Module): Objek model yang arsitekturnya SAMA\n",
    "                                        (misal: Cnn_interpolation(k_feature=64)).\n",
    "        path_to_weights (str): Path ke file .pth yang disimpan.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Model dengan bobot yang sudah dimuat.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # baca file csv\n",
    "    df = pd.read_csv(path_hist)\n",
    "    path_to_weights = \"/kaggle/working/model_checkpoints/\"+df['nama_model'].iloc[-1]\n",
    "    \n",
    "    \n",
    "    # Muat state_dict dari file\n",
    "    # map_location=device memastikan bobot dimuat ke device yang benar\n",
    "    state_dict = torch.load(path_to_weights, map_location=device)\n",
    "    \n",
    "    # Masukkan state_dict (bobot) ke dalam arsitektur model\n",
    "    model_architecture.load_state_dict(state_dict)\n",
    "    \n",
    "    # Pindahkan model ke device\n",
    "    model_architecture.to(device)\n",
    "    \n",
    "    # PENTING: Set model ke mode evaluasi/inferensi\n",
    "    # Ini menonaktifkan layer seperti Dropout atau BatchNorm (jika ada)\n",
    "    model_architecture.eval() \n",
    "    \n",
    "    print(f\"Model berhasil dimuat dari {path_to_weights}\")\n",
    "    return model_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbbe5c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:02:52.213827Z",
     "iopub.status.busy": "2025-11-12T07:02:52.213624Z",
     "iopub.status.idle": "2025-11-12T07:02:52.262888Z",
     "shell.execute_reply": "2025-11-12T07:02:52.262383Z"
    },
    "papermill": {
     "duration": 0.085826,
     "end_time": "2025-11-12T07:02:52.263954",
     "exception": false,
     "start_time": "2025-11-12T07:02:52.178128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def pilih_file_weight():\n",
    "    \"\"\"\n",
    "    Membuka file explorer untuk memilih file .pth.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # sembunyikan jendela utama Tkinter\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Pilih file weight (.pth)\",\n",
    "        filetypes=[(\"PyTorch model\", \"*.pth\"), (\"Semua file\", \"*.*\")]\n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# ==== Contoh penggunaan ====\n",
    "# Misalnya kamu sudah punya arsitektur model\n",
    "# model = Cnn_interpolation(k_feature=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe18663",
   "metadata": {
    "papermill": {
     "duration": 0.035565,
     "end_time": "2025-11-12T07:02:52.408830",
     "exception": false,
     "start_time": "2025-11-12T07:02:52.373265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "511394dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:02:52.480361Z",
     "iopub.status.busy": "2025-11-12T07:02:52.480074Z",
     "iopub.status.idle": "2025-11-12T07:02:52.820572Z",
     "shell.execute_reply": "2025-11-12T07:02:52.819965Z"
    },
    "papermill": {
     "duration": 0.377887,
     "end_time": "2025-11-12T07:02:52.821905",
     "exception": false,
     "start_time": "2025-11-12T07:02:52.444018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "PATH = kagglehub.dataset_download(\"mcparadip/anime-faces-waifu2x\")\n",
    "\n",
    "# # print(\"Path to dataset files:\", path)\n",
    "# PATH = \"/home/kanza/Dokumen/file/D2L/transformer/anim/archive\" # <-- GANTI INI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1fbb8",
   "metadata": {
    "papermill": {
     "duration": 0.040073,
     "end_time": "2025-11-12T07:02:52.900783",
     "exception": false,
     "start_time": "2025-11-12T07:02:52.860710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae519b",
   "metadata": {
    "papermill": {
     "duration": 0.036501,
     "end_time": "2025-11-12T07:02:52.974089",
     "exception": false,
     "start_time": "2025-11-12T07:02:52.937588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "309ef024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:02:53.047319Z",
     "iopub.status.busy": "2025-11-12T07:02:53.046681Z",
     "iopub.status.idle": "2025-11-12T07:02:53.050501Z",
     "shell.execute_reply": "2025-11-12T07:02:53.049928Z"
    },
    "papermill": {
     "duration": 0.041724,
     "end_time": "2025-11-12T07:02:53.051502",
     "exception": false,
     "start_time": "2025-11-12T07:02:53.009778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "# # Tentukan path ke folder gambar Anda\n",
    "# PATH_KE_FOLDER_GAMBAR = PATH # <-- GANTI INI\n",
    "# BATCH_SIZE = 1\n",
    "# NUM_EPOCHS = 1\n",
    "# K_FEATURE = 96\n",
    "\n",
    "# # --- MODIFIKASI DIMULAI DI SINI ---\n",
    "\n",
    "# # 1. Buat dataset Lengkap\n",
    "# full_dataset = SuperResolutionDataset(image_folder=PATH_KE_FOLDER_GAMBAR)\n",
    "# print(f\"Ukuran dataset penuh: {len(full_dataset)}\")\n",
    "\n",
    "# # 2. Tentukan jumlah gambar yang ingin digunakan (misal, 100)\n",
    "# num_images_to_use = 2\n",
    "# indices = list(range(num_images_to_use))\n",
    "\n",
    "# # 3. Buat SUBSET dari dataset penuh\n",
    "# subset_dataset = Subset(full_dataset, indices)\n",
    "# print(f\"Ukuran subset yang akan digunakan: {len(subset_dataset)}\")\n",
    "\n",
    "# # 4. Tentukan ukuran split (sekarang berdasarkan subset_dataset)\n",
    "# train_size = int(0.8 * len(subset_dataset)) # Ini akan menjadi 80\n",
    "# val_size = len(subset_dataset) - train_size   # Ini akan menjadi 20\n",
    "\n",
    "# # 5. Lakukan split pada SUBSET tersebut\n",
    "# train_dataset, val_dataset = random_split(subset_dataset, [train_size, val_size])\n",
    "\n",
    "# print(f\"Ukuran data training: {len(train_dataset)}\")\n",
    "# print(f\"Ukuran data validasi: {len(val_dataset)}\")\n",
    "\n",
    "# # --- MODIFIKASI SELESAI ---\n",
    "\n",
    "# # 6. Buat data loader train dan val (kode ini tetap sama)\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=True, \n",
    "#     num_workers=4,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False, # Tidak perlu di-shuffle untuk validasi\n",
    "#     num_workers=4,\n",
    "#     pin_memory=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8993c",
   "metadata": {
    "papermill": {
     "duration": 0.035329,
     "end_time": "2025-11-12T07:02:53.121829",
     "exception": false,
     "start_time": "2025-11-12T07:02:53.086500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e29b0ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:02:53.193739Z",
     "iopub.status.busy": "2025-11-12T07:02:53.193554Z",
     "iopub.status.idle": "2025-11-12T07:02:53.376920Z",
     "shell.execute_reply": "2025-11-12T07:02:53.376115Z"
    },
    "papermill": {
     "duration": 0.22094,
     "end_time": "2025-11-12T07:02:53.378044",
     "exception": false,
     "start_time": "2025-11-12T07:02:53.157104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21551\n",
      "Ukuran data training: 17240\n",
      "Ukuran data validasi: 4311\n"
     ]
    }
   ],
   "source": [
    "# Tentukan path ke folder gambar Anda\n",
    "PATH_KE_FOLDER_GAMBAR = PATH # <-- GANTI INI\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "K_FEATURE = 96\n",
    "\n",
    "# 1. Buat dataset Lengkap\n",
    "full_dataset =  SuperResolutionDataset(image_folder=PATH_KE_FOLDER_GAMBAR)\n",
    "print(len(full_dataset))\n",
    "\n",
    "# 2. Tentukan ukuran split \n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "# 3.Lakukan split\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Ukuran data training: {len(train_dataset)}\")\n",
    "print(f\"Ukuran data validasi: {len(val_dataset)}\")\n",
    "\n",
    "# 4. buat data loader train dan val\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # Tidak perlu di-shuffle untuk validasi\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7534185e",
   "metadata": {
    "papermill": {
     "duration": 0.035872,
     "end_time": "2025-11-12T07:02:53.449581",
     "exception": false,
     "start_time": "2025-11-12T07:02:53.413709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8564f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T07:02:53.521739Z",
     "iopub.status.busy": "2025-11-12T07:02:53.521547Z",
     "iopub.status.idle": "2025-11-12T08:24:58.774046Z",
     "shell.execute_reply": "2025-11-12T08:24:58.773165Z"
    },
    "papermill": {
     "duration": 4925.290269,
     "end_time": "2025-11-12T08:24:58.775478",
     "exception": false,
     "start_time": "2025-11-12T07:02:53.485209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:02<00:00, 216MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/vgg.pth\n",
      "Mulai Training di device: cuda | run_id=2025-11-12_07-02-58 | start_epoch_global=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 [Train]: 100%|██████████| 17240/17240 [1:15:17<00:00,  3.82it/s, train_loss=0.127]\n",
      "Epoch 1/1 [Val]: 100%|██████████| 4311/4311 [06:43<00:00, 10.68it/s, val_loss=0.108, lpips=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] e1 | val_loss=0.114601 | PSNR=16.1219 | SSIM=0.3243 | LPIPS=0.5409\n",
      "[SAVE] model_checkpoints/model_2025-11-12_07-02-58_e1.pth\n",
      "Training Selesai.\n",
      "\n",
      "--- Hasil History Training ---\n",
      "{\n",
      "  \"train_loss\": [\n",
      "    1.526775916580007\n",
      "  ],\n",
      "  \"val_loss\": [\n",
      "    0.11460102608587702\n",
      "  ],\n",
      "  \"val_psnr\": [\n",
      "    16.12189292907715\n",
      "  ],\n",
      "  \"val_ssim\": [\n",
      "    0.3243226408958435\n",
      "  ],\n",
      "  \"val_lpips\": [\n",
      "    0.5409156868834419\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "# ================================================== inisialisasi model ==========================================\n",
    "model = RIR_interpolation(\n",
    "    N=1,\n",
    "    C=3,\n",
    "    H=128,\n",
    "    W=128,\n",
    "    k_feature=96,\n",
    "    j_group=6,\n",
    "    j_blocks=6)\n",
    "\n",
    "model = model.to(device)           # ⬅️ pindahkan SEMUA parameter model ke GPU\n",
    "\n",
    "# tentukan loss function\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# menentukan optimizer \n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "\n",
    "# ================================= Trainig ===================================================================\n",
    "# 6. Mulai Training dan TANGKAP output history\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,  # Berikan val_loader di sini\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    save_dir=\"model_checkpoints\",\n",
    "    continue_global_epoch=False\n",
    ")\n",
    "\n",
    "# 7. Tampilkan hasil history\n",
    "print(\"\\n--- Hasil History Training ---\")\n",
    "import json\n",
    "print(json.dumps(history, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a65c94cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T08:25:02.328839Z",
     "iopub.status.busy": "2025-11-12T08:25:02.328536Z",
     "iopub.status.idle": "2025-11-12T08:25:02.365660Z",
     "shell.execute_reply": "2025-11-12T08:25:02.364895Z"
    },
    "papermill": {
     "duration": 1.818227,
     "end_time": "2025-11-12T08:25:02.366794",
     "exception": false,
     "start_time": "2025-11-12T08:25:00.548567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>epoch_in_run</th>\n",
       "      <th>run_id</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_psnr</th>\n",
       "      <th>val_ssim</th>\n",
       "      <th>val_lpips</th>\n",
       "      <th>nama_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-11-12_07-02-58</td>\n",
       "      <td>1.526776</td>\n",
       "      <td>0.114601</td>\n",
       "      <td>16.121893</td>\n",
       "      <td>0.324323</td>\n",
       "      <td>0.540916</td>\n",
       "      <td>model_2025-11-12_07-02-58_e1.pth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  epoch_in_run               run_id  train_loss  val_loss   val_psnr  \\\n",
       "0      1             1  2025-11-12_07-02-58    1.526776  0.114601  16.121893   \n",
       "\n",
       "   val_ssim  val_lpips                        nama_model  \n",
       "0  0.324323   0.540916  model_2025-11-12_07-02-58_e1.pth  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/kaggle/working/histor.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2576dea",
   "metadata": {
    "papermill": {
     "duration": 1.733588,
     "end_time": "2025-11-12T08:25:05.842339",
     "exception": false,
     "start_time": "2025-11-12T08:25:04.108751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69bb3c5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T08:25:09.326341Z",
     "iopub.status.busy": "2025-11-12T08:25:09.326037Z",
     "iopub.status.idle": "2025-11-12T08:25:09.329887Z",
     "shell.execute_reply": "2025-11-12T08:25:09.329313Z"
    },
    "papermill": {
     "duration": 1.753362,
     "end_time": "2025-11-12T08:25:09.330908",
     "exception": false,
     "start_time": "2025-11-12T08:25:07.577546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = RIR_interpolation(\n",
    "#     N=1,\n",
    "#     C=3,\n",
    "#     H=128,\n",
    "#     W=128,\n",
    "#     k_feature=96,\n",
    "#     j_group=6,\n",
    "#     j_blocks=6)\n",
    "\n",
    "\n",
    "# load_model(model, '/kaggle/working/histor.csv')\n",
    "\n",
    "# model = model.to(device)           # ⬅️ pindahkan SEMUA parameter model ke GPU\n",
    "\n",
    "# # tentukan loss function\n",
    "# criterion = nn.L1Loss()\n",
    "\n",
    "# # menentukan optimizer \n",
    "# optimizer = optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "\n",
    "# # ================================= Trainig ===================================================================\n",
    "# # 6. Mulai Training dan TANGKAP output history\n",
    "# history = train_model(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,  # Berikan val_loader di sini\n",
    "#     criterion=criterion,\n",
    "#     optimizer=optimizer,\n",
    "#     num_epochs=3,\n",
    "#     save_dir=\"model_checkpoints\",\n",
    "#     continue_global_epoch=True\n",
    "# )\n",
    "\n",
    "# # 7. Tampilkan hasil history\n",
    "# print(\"\\n--- Hasil History Training ---\")\n",
    "# import json\n",
    "# print(json.dumps(history, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "731ca616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T08:25:12.833447Z",
     "iopub.status.busy": "2025-11-12T08:25:12.832752Z",
     "iopub.status.idle": "2025-11-12T08:25:12.844119Z",
     "shell.execute_reply": "2025-11-12T08:25:12.843483Z"
    },
    "papermill": {
     "duration": 1.679697,
     "end_time": "2025-11-12T08:25:12.845286",
     "exception": false,
     "start_time": "2025-11-12T08:25:11.165589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>epoch_in_run</th>\n",
       "      <th>run_id</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_psnr</th>\n",
       "      <th>val_ssim</th>\n",
       "      <th>val_lpips</th>\n",
       "      <th>nama_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-11-12_07-02-58</td>\n",
       "      <td>1.526776</td>\n",
       "      <td>0.114601</td>\n",
       "      <td>16.121893</td>\n",
       "      <td>0.324323</td>\n",
       "      <td>0.540916</td>\n",
       "      <td>model_2025-11-12_07-02-58_e1.pth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  epoch_in_run               run_id  train_loss  val_loss   val_psnr  \\\n",
       "0      1             1  2025-11-12_07-02-58    1.526776  0.114601  16.121893   \n",
       "\n",
       "   val_ssim  val_lpips                        nama_model  \n",
       "0  0.324323   0.540916  model_2025-11-12_07-02-58_e1.pth  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/kaggle/working/histor.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ee6d7ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T08:25:16.314451Z",
     "iopub.status.busy": "2025-11-12T08:25:16.314167Z",
     "iopub.status.idle": "2025-11-12T08:25:16.318284Z",
     "shell.execute_reply": "2025-11-12T08:25:16.317543Z"
    },
    "papermill": {
     "duration": 1.736775,
     "end_time": "2025-11-12T08:25:16.319437",
     "exception": false,
     "start_time": "2025-11-12T08:25:14.582662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('UTC', 'UTC')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(time.tzname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d536280",
   "metadata": {
    "papermill": {
     "duration": 1.726236,
     "end_time": "2025-11-12T08:25:19.775612",
     "exception": false,
     "start_time": "2025-11-12T08:25:18.049376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 671155,
     "sourceId": 1181323,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5058.000648,
   "end_time": "2025-11-12T08:25:23.952684",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-12T07:01:05.952036",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
