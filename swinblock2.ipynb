{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9468d674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:18:23.036894Z",
     "iopub.status.busy": "2025-11-13T06:18:23.036242Z",
     "iopub.status.idle": "2025-11-13T06:19:29.501306Z",
     "shell.execute_reply": "2025-11-13T06:19:29.500570Z"
    },
    "papermill": {
     "duration": 66.472329,
     "end_time": "2025-11-13T06:19:29.502901",
     "exception": false,
     "start_time": "2025-11-13T06:18:23.030572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet lpips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2aefcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:29.542198Z",
     "iopub.status.busy": "2025-11-13T06:19:29.541938Z",
     "iopub.status.idle": "2025-11-13T06:19:43.218242Z",
     "shell.execute_reply": "2025-11-13T06:19:43.217387Z"
    },
    "papermill": {
     "duration": 13.698178,
     "end_time": "2025-11-13T06:19:43.219851",
     "exception": false,
     "start_time": "2025-11-13T06:19:29.521673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "from tqdm import tqdm # Opsional, tapi sangat disarankan untuk progress bar\n",
    "from torch.utils.data import random_split\n",
    "from math import floor\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import csv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- import LPIPS ---\n",
    "try:\n",
    "    import lpips  # pip install lpips\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Package 'lpips' belum terpasang. Jalankan: pip install lpips\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de250899",
   "metadata": {
    "papermill": {
     "duration": 0.0178,
     "end_time": "2025-11-13T06:19:43.256179",
     "exception": false,
     "start_time": "2025-11-13T06:19:43.238379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fungsi train, custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060426ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:43.293472Z",
     "iopub.status.busy": "2025-11-13T06:19:43.292806Z",
     "iopub.status.idle": "2025-11-13T06:19:44.701817Z",
     "shell.execute_reply": "2025-11-13T06:19:44.701183Z"
    },
    "papermill": {
     "duration": 1.429182,
     "end_time": "2025-11-13T06:19:44.703191",
     "exception": false,
     "start_time": "2025-11-13T06:19:43.274009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, csv, time, shutil\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "import lpips  # pip install lpips\n",
    "\n",
    "def _read_last_epoch(csv_path):\n",
    "    if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if 'epoch' in df.columns and pd.api.types.is_numeric_dtype(df['epoch']):\n",
    "            ep = df['epoch'].dropna()\n",
    "            return int(ep.max()) if len(ep) else 0\n",
    "        return 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def _prepare_csv(seed_csv, path_write):\n",
    "    \"\"\"Jika path_write belum ada dan seed_csv ada, salin ke working.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path_write), exist_ok=True)\n",
    "    if (seed_csv is not None and os.path.exists(seed_csv)) and (not os.path.exists(path_write)):\n",
    "        shutil.copyfile(seed_csv, path_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d454507b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:44.741453Z",
     "iopub.status.busy": "2025-11-13T06:19:44.741024Z",
     "iopub.status.idle": "2025-11-13T06:19:44.753950Z",
     "shell.execute_reply": "2025-11-13T06:19:44.753394Z"
    },
    "papermill": {
     "duration": 0.033125,
     "end_time": "2025-11-13T06:19:44.755074",
     "exception": false,
     "start_time": "2025-11-13T06:19:44.721949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs,\n",
    "    save_dir=\"model_checkpoints\",\n",
    "    seed_csv=\"/kaggle/input/history1/histor.csv\",      # <-- sumber read-only (opsional)\n",
    "    path_write=\"/kaggle/working/histor.csv\",           # <-- tujuan APPEND (writable)\n",
    "    continue_global_epoch=True\n",
    "):\n",
    "    # siapkan folder writable dan CSV tujuan\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    _prepare_csv(seed_csv, path_write)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # metrics\n",
    "    psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    lpips_fn = lpips.LPIPS(net='vgg').to(device).eval()\n",
    "\n",
    "    def to_minus1_1(x): return x*2.0 - 1.0\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_psnr': [], 'val_ssim': [], 'val_lpips': []}\n",
    "\n",
    "    run_id = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    start_epoch_global = _read_last_epoch(path_write) if continue_global_epoch else 0\n",
    "    print(f\"Mulai Training di {device} | run_id={run_id} | start_epoch_global={start_epoch_global}\")\n",
    "    print(f\"CSV tulis: {path_write}\")\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        epoch_in_run = e + 1\n",
    "        epoch_global = start_epoch_global + epoch_in_run\n",
    "\n",
    "        # ===== TRAIN =====\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for lr_images, hr_images in tqdm(train_loader, desc=f\"Epoch {epoch_in_run}/{num_epochs} [Train]\"):\n",
    "            lr_images = lr_images.to(device); hr_images = hr_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(lr_images)\n",
    "            loss = criterion(pred, hr_images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * lr_images.size(0)\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "\n",
    "        # ===== VAL =====\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        psnr_metric.reset(); ssim_metric.reset()\n",
    "        lpips_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for lr_images, hr_images in tqdm(val_loader, desc=f\"Epoch {epoch_in_run}/{num_epochs} [Val]\"):\n",
    "                lr_images = lr_images.to(device); hr_images = hr_images.to(device)\n",
    "                pred = model(lr_images)\n",
    "                vloss = criterion(pred, hr_images)\n",
    "                running_val_loss += vloss.item() * lr_images.size(0)\n",
    "\n",
    "                psnr_metric.update(pred, hr_images)\n",
    "                ssim_metric.update(pred, hr_images)\n",
    "\n",
    "                lp = lpips_fn(to_minus1_1(pred), to_minus1_1(hr_images)).mean().item()\n",
    "                lpips_sum += lp * lr_images.size(0)\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_psnr = psnr_metric.compute().item()\n",
    "        epoch_ssim = ssim_metric.compute().item()\n",
    "        epoch_lpips = lpips_sum / len(val_loader.dataset)\n",
    "\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_psnr'].append(epoch_psnr)\n",
    "        history['val_ssim'].append(epoch_ssim)\n",
    "        history['val_lpips'].append(epoch_lpips)\n",
    "\n",
    "        print(f\"[RESULT] e{epoch_in_run} | val_loss={epoch_val_loss:.6f} | PSNR={epoch_psnr:.4f} | SSIM={epoch_ssim:.4f} | LPIPS={epoch_lpips:.4f}\")\n",
    "\n",
    "        # ===== SAVE CHECKPOINT (FIX) =====\n",
    "        ckpt_name = f\"model_{run_id}_e{epoch_global}.pth\"   # simpan nama + epoch_global\n",
    "        ckpt_path = os.path.join(save_dir, ckpt_name)\n",
    "        torch.save({\n",
    "            \"epoch\": epoch_global,                  # <--- pakai epoch_global\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optim_state\": optimizer.state_dict(),\n",
    "        }, ckpt_path)\n",
    "        print(f\"[SAVE] {ckpt_path}\")\n",
    "\n",
    "        # ===== APPEND ke CSV (SELALU ke path_write di /kaggle/working) =====\n",
    "        is_new = not os.path.exists(path_write) or os.path.getsize(path_write) == 0\n",
    "        with open(path_write, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.DictWriter(\n",
    "                f,\n",
    "                fieldnames=[\n",
    "                    \"epoch\",\"epoch_in_run\",\"run_id\",\n",
    "                    \"train_loss\",\"val_loss\",\"val_psnr\",\"val_ssim\",\"val_lpips\",\"nama_model\"\n",
    "                ]\n",
    "            )\n",
    "            if is_new:\n",
    "                w.writeheader()\n",
    "            w.writerow({\n",
    "                \"epoch\":        epoch_global,\n",
    "                \"epoch_in_run\": epoch_in_run,\n",
    "                \"run_id\":       run_id,\n",
    "                \"train_loss\":   float(epoch_train_loss),\n",
    "                \"val_loss\":     float(epoch_val_loss),\n",
    "                \"val_psnr\":     float(epoch_psnr),\n",
    "                \"val_ssim\":     float(epoch_ssim),\n",
    "                \"val_lpips\":    float(epoch_lpips),\n",
    "                \"nama_model\":   ckpt_name          # <--- cocok dengan yang disimpan\n",
    "            })\n",
    "\n",
    "    print(\"Training Selesai.\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ba6e9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:44.792303Z",
     "iopub.status.busy": "2025-11-13T06:19:44.792043Z",
     "iopub.status.idle": "2025-11-13T06:19:44.797709Z",
     "shell.execute_reply": "2025-11-13T06:19:44.797109Z"
    },
    "papermill": {
     "duration": 0.025672,
     "end_time": "2025-11-13T06:19:44.798882",
     "exception": false,
     "start_time": "2025-11-13T06:19:44.773210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SuperResolutionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset kustom untuk super-resolution.\n",
    "    Akan memuat gambar 256x256 (HR) dan secara otomatis\n",
    "    membuat versi 128x128 (LR) sebagai input.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_folder):\n",
    "        self.root_dir = image_folder\n",
    "        # Dapatkan daftar semua nama file gambar di folder\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        # Definisikan transform untuk HR (hanya konversi ke Tensor)\n",
    "        self.hr_transform = T.ToTensor()\n",
    "        \n",
    "        # Definisikan transform untuk LR (Resize ke 128x128 + konversi ke Tensor)\n",
    "        self.lr_transform = T.Compose([\n",
    "            T.Resize((128, 128), interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        # Mengembalikan jumlah total gambar\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Dapatkan path lengkap ke satu gambar\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        \n",
    "        # 2. Buka gambar HR (256x256) menggunakan PIL\n",
    "        #    Gunakan .convert('RGB') untuk memastikan gambar punya 3 channel\n",
    "        hr_image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 3. Buat tensor HR dan LR dari gambar yang sama\n",
    "        lr_tensor = self.lr_transform(hr_image)\n",
    "        hr_tensor = self.hr_transform(hr_image)\n",
    "        \n",
    "        # 4. Kembalikan pasangan (input, target)\n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7b461",
   "metadata": {
    "papermill": {
     "duration": 0.02808,
     "end_time": "2025-11-13T06:19:44.850563",
     "exception": false,
     "start_time": "2025-11-13T06:19:44.822483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ARSITEKTUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02dfa28f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:44.895947Z",
     "iopub.status.busy": "2025-11-13T06:19:44.895668Z",
     "iopub.status.idle": "2025-11-13T06:19:44.918741Z",
     "shell.execute_reply": "2025-11-13T06:19:44.918046Z"
    },
    "papermill": {
     "duration": 0.043585,
     "end_time": "2025-11-13T06:19:44.919837",
     "exception": false,
     "start_time": "2025-11-13T06:19:44.876252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class blocks (nn.Module):\n",
    "\n",
    "    def __init__(self, k_feature, N,H,W,C,j, head):\n",
    "        super().__init__()\n",
    "        self.j = j\n",
    "        self.N, self.C, self.H, self.W = N,C,H,W\n",
    "        self.k_feature = k_feature\n",
    "        self.head = head\n",
    "        self.dk = self.k_feature//self.head # dmodel = k_feature\n",
    "        self.n_win = (self.H//self.j) * (self.W//self.j)\n",
    "        self.rasio_ekspansi = 4\n",
    "    \n",
    "    # ============================ body Rir =====================================\n",
    "        # normalisasi\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=[self.k_feature])\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=[self.k_feature])\n",
    "\n",
    "        # Linear projections\n",
    "        self.wq = nn.Parameter(torch.empty(self.k_feature, self.k_feature))\n",
    "        self.wk = nn.Parameter(torch.empty(self.k_feature, self.k_feature))\n",
    "        self.wv = nn.Parameter(torch.empty(self.k_feature, self.k_feature))\n",
    "        self.wo = nn.Parameter(torch.empty(self.k_feature, self.k_feature))\n",
    "        self.bt = nn.Parameter(torch.zeros(self.head, self.j*self.j, self.j*self.j))\n",
    "\n",
    "        # MLP\n",
    "        self.w1 = nn.Parameter(torch.empty(self.k_feature, self.k_feature * self.rasio_ekspansi))\n",
    "        self.b1 = nn.Parameter(torch.zeros([self.k_feature*self.rasio_ekspansi]))\n",
    "        self.w2 = nn.Parameter(torch.empty(self.k_feature*self.rasio_ekspansi, self.k_feature))\n",
    "        self.b2 = nn.Parameter(torch.zeros([self.k_feature]))\n",
    "\n",
    "        # Panggil fungsi inisialisasi\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # === Xavier initialization untuk layer linear ===\n",
    "        nn.init.xavier_uniform_(self.wq)\n",
    "        nn.init.xavier_uniform_(self.wk)\n",
    "        nn.init.xavier_uniform_(self.wv)\n",
    "        nn.init.xavier_uniform_(self.wo)\n",
    "        nn.init.xavier_uniform_(self.w1)\n",
    "        nn.init.xavier_uniform_(self.w2)\n",
    "\n",
    "        # Bias nol sudah oke (default)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.zeros_(self.b2)\n",
    "\n",
    "        # Bisa juga He init jika ingin lebih cocok untuk ReLU:\n",
    "        # nn.init.kaiming_normal_(self.w1, nonlinearity='relu')\n",
    "        # nn.init.kaiming_normal_(self.w2, nonlinearity='relu')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \n",
    "        # input kedala fungsi ini x [8,96,128,128]\n",
    "        \n",
    "        # =================================================--=======================================================\n",
    "        # ============================================== MSA =======================================================\n",
    "        # =================================================--=======================================================\n",
    "        \n",
    "        # menukra ukuran dimensi (N,C,H,W) => (N,H,W,C)\n",
    "        xp = x.permute(0,2,3,1)\n",
    "        \n",
    "        # normalisasi input\n",
    "       \n",
    "        xln1 = self.layernorm1(xp)\n",
    "        N_dyn, H_dyn, W_dyn, C_dyn = xln1.shape\n",
    "\n",
    "        # window partition. \n",
    "        # reshape (N,H,W,C) => ((N X n_win),j,j,k_feature)\n",
    "        # 1.view potong H dan W menjadi (numwin_H, j) dan (NumWin_W, j)\n",
    "        xr = xln1.view(N_dyn, H_dyn//self.j, self.j, W_dyn//self.j, self.j, C_dyn)\n",
    "        \n",
    "        # 2. permute [N, H//J, W//J, j,j, k_feature]\n",
    "        xwin = xr.permute(0,1,3,2,4,5).contiguous()\n",
    "\n",
    "        # 3.view gabugnkan semua jendela kedalam dimenci batch\n",
    "        # [N * num_wind * numwind , j,j, C]\n",
    "        xwin = xwin.view(-1,self.j,self.j,C_dyn)\n",
    "        # xr = xln1.reshape(self.N * self.n_win, self.j, self.j, self.k_feature)\n",
    "\n",
    "        # flatten ((N X n_win),j,j,C) => ((N X n_win),j*j,C)  \n",
    "        xf = xwin.reshape(-1, self.j * self.j, self.k_feature)\n",
    "\n",
    "        # self attention\n",
    "        # mencari Q,K,V\n",
    "        Q = xf @ self.wq\n",
    "        K = xf @ self.wk\n",
    "        V = xf @ self.wv\n",
    "\n",
    "        # split heads mengubah ukuran Q,K,V [2048,64,96] => menjadi [2048,64,6,16]\n",
    "        Q = Q.reshape((-1, self.j * self.j, self.head, self.k_feature//self.head))\n",
    "        K = K.reshape((-1, self.j * self.j, self.head, self.k_feature//self.head))\n",
    "        V = V.reshape((-1, self.j * self.j, self.head, self.k_feature//self.head))\n",
    "\n",
    "        # permute dari bentuk [2048,64,6,16] => [2048,6,64,16]\n",
    "        Q = Q.permute(0,2,1,3)\n",
    "        K = K.permute(0,2,1,3)\n",
    "        V = V.permute(0,2,1,3)\n",
    "\n",
    "        # SCALED dot product attention\n",
    "        score = Q.matmul(K.transpose(-2,-1))\n",
    "        scaling = score/(self.dk ** 0.5)\n",
    "        B = scaling + self.bt\n",
    "        P = F.softmax(B, dim=-1)\n",
    "        O = P @ V\n",
    "\n",
    "        #permute dari [2048,6,64,16] => [2048,64,6,16]\n",
    "        O = O.permute(0,2,1,3)\n",
    "        \n",
    "        #reshape menggabungkan head dari [2048,64,6,16] => [2048,64,96]\n",
    "        O = O.reshape(-1, self.j * self.j, self.k_feature )\n",
    "\n",
    "        # MSA_out\n",
    "        MSA_out = O @ self.wo\n",
    "\n",
    "        # mengubah kembali ke bentuk input awal [N,C,H,W]\n",
    "        # 1. Reshape ke [256, 8, 8, 96]\n",
    "        MSA_out = MSA_out.view(-1, self.j, self.j, self.k_feature) \n",
    "        \n",
    "        # 2. Reshape ke [N_dyn, NumWin_H, NumWin_W, j, j, C]\n",
    "        MSA_out = MSA_out.view(N_dyn, H_dyn // self.j, W_dyn // self.j, self.j, self.j, self.k_feature)\n",
    "        \n",
    "        # 3. Permute\n",
    "        MSA_out = MSA_out.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        \n",
    "        # 4. View ke [N_dyn, H_dyn, W_dyn, C]\n",
    "        MSA_out = MSA_out.view(N_dyn, H_dyn, W_dyn, self.k_feature)\n",
    "\n",
    "        # # reshape dari ukuran [2048,64,96] => [2048,8,8,96]\n",
    "        # MSA_out = MSA_out.reshape(self.N * self.n_win,self.j, self.j, self.k_feature)\n",
    "\n",
    "        # # reshape dari ukuran [2048,8,8,96] => [8,256,8,8,96]\n",
    "        # MSA_out = MSA_out.reshape(self.N, self.n_win, self.j, self.j, self.k_feature)\n",
    "\n",
    "        # # reshape dari ukuran [8,256,8,8,96] => [8,128,128,96]\n",
    "        # MSA_out = MSA_out.reshape(self.N, self.H, self.W, self.k_feature)\n",
    "\n",
    "        # permute untuk mengembalikan ke bentuk awal N,C,H,W\n",
    "        MSA_out = MSA_out.permute(0,-1,1,2)\n",
    "\n",
    "        #============ RESIDUAL =======================\n",
    "        xmsa = x + MSA_out # [N,k_feature, H,W] [8.96,128,128]\n",
    "\n",
    "        # =================================================--=======================================================\n",
    "        # ============================================== MLP =======================================================\n",
    "        # =================================================--=======================================================\n",
    "\n",
    "        # input x [N,k_feature, H,W] [8.96,128,128]\n",
    "        x2 = xmsa.clone()\n",
    "       # menukra ukuran dimensi (N,C,H,W) => (N,H,W,C)\n",
    "        xp = x2.permute(0,2,3,1)\n",
    "    \n",
    "        # normalisasi input\n",
    "        xln = self.layernorm2(xp)\n",
    "\n",
    "        # Multi Layer Perceptron \n",
    "        # layer1 (8,128,128,96) => (8,128,128,384)\n",
    "        x2 = xln @ self.w1 + self.b1\n",
    "        # aktivasi\n",
    "        x2 = F.gelu(x2)\n",
    "\n",
    "        # layer2 (8,128,128,384) => (8,128,128,96)\n",
    "        x2 = x2 @ self.w2 + self.b2\n",
    "\n",
    "        # permutasi\n",
    "        x2 = x2.permute(0,3,1,2)\n",
    "\n",
    "        # residual\n",
    "        x_out = x2 + xmsa\n",
    "        return x_out\n",
    "\n",
    "class group(nn.Module):\n",
    "    def __init__(self, k_feature, N,H,W,C,j, head, j_blocks):\n",
    "        super().__init__()\n",
    "        self.j = j\n",
    "        self.N, self.C, self.H, self.W = N,C,H,W\n",
    "        self.k_feature = k_feature\n",
    "        self.head = head\n",
    "        self.dk = self.k_feature//self.head # dmodel = k_feature\n",
    "        self.n_win = self.H//self.j * self.W/self.j\n",
    "        \n",
    "        self.bloks = nn.ModuleList([\n",
    "            blocks(k_feature=self.k_feature, N=self.N,H=self.H,W=self.W,C=self.C,j=self.j, head = self.head) for _ in range(j_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        xin = x.clone()\n",
    "        for blk in self.bloks:\n",
    "            x = blk(x)\n",
    "        # residual \\\n",
    "        x_out = xin + x\n",
    "        return x_out\n",
    "\n",
    "    \n",
    "\n",
    "class RIR_interpolation(nn.Module):\n",
    "\n",
    "    def __init__(self, N,C,H,W,  k_feature, j_group, j_blocks):\n",
    "        super().__init__()\n",
    "        self.N, self.C, self.H, self.W = N,C,H,W\n",
    "        # self.groups = groups\n",
    "        self.blocks = j_blocks\n",
    "        self.j = 8  # ukuran window\n",
    "        self.k_feature = k_feature\n",
    "        self.head = 6\n",
    "        self.dk = self.k_feature//self.head # dmodel = k_feature\n",
    "        self.j_group = j_group\n",
    "        \n",
    "        self.n_win = self.H//self.j * self.W//self.j\n",
    "        \n",
    "        # [out_channels, in_channels, kernel, kernel]\n",
    "\n",
    "        # ========================== ekstraksi fitur ================================\n",
    "        self.w0 = nn.Parameter(torch.empty(self.k_feature, 3, 3, 3))\n",
    "        self.w1 = nn.Parameter(torch.empty(self.k_feature * 4, k_feature, 3, 3))\n",
    "        self.w2 = nn.Parameter(torch.empty(3, self.k_feature, 3, 3))\n",
    "        self.b0 = nn.Parameter(torch.zeros(self.k_feature))\n",
    "        self.b1 = nn.Parameter(torch.zeros(self.k_feature * 4))\n",
    "        self.b2 = nn.Parameter(torch.zeros(3))\n",
    "\n",
    "\n",
    "        # body \n",
    "        self.body = nn.ModuleList([\n",
    "            group(self.k_feature, self.N, self.H, self.W, self.C, self.j, self.head, 6) for _ in range(self.j_group)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.pixshuffle = nn.PixelShuffle(upscale_factor=2)\n",
    "        self.init_weights()\n",
    "    \n",
    "\n",
    "    def init_weights(self):\n",
    "        # He initialization untuk convolution\n",
    "        nn.init.kaiming_normal_(self.w0, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.w1, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.w2, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.b0)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.zeros_(self.b2)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # =================Ekstraksi Fitur===========================\n",
    "        # input awal X [N,C,H,W] [8,3,128,128]\n",
    "        d = F.conv2d(x, self.w0, bias=self.b0, padding=1)\n",
    "        d = F.relu(d) # [8,96,128,128]\n",
    "        # output = X = [N,k_feature,H,W] [8,96,128,128]\n",
    "\n",
    "        \n",
    "        # ================== body rir ==============================\n",
    "        #  input awal X [N,k_featur,H,W] = [8,96,128,128]\n",
    "        \n",
    "        # b = self.body.forward(d)\n",
    "        din = d.clone()\n",
    "        for grp in self.body:\n",
    "            d = grp(d)\n",
    "\n",
    "        # residual\n",
    "        b = din + d\n",
    "            \n",
    "\n",
    "        # output sama dengan input X = X [N,k_featur,H,W] = [8,96,128,128]\n",
    "\n",
    "        \n",
    "        # =============== Upsampler/pixxel shuffle ==================\n",
    "        # input awal X [N,k_featur,H,W] = [8,96,128,128]\n",
    "        d = F.conv2d(b, self.w1, bias=self.b1, padding=1)\n",
    "        d = F.relu(d)\n",
    "        o = self.pixshuffle(d)\n",
    "\n",
    "        # Rekonstruksi\n",
    "        xsr = F.conv2d(o, self.w2, bias=self.b2, padding=1)\n",
    "        # output X [N.C,H*2,W*2]  [8,3,256,256]\n",
    "        \n",
    "        return xsr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00eae9f",
   "metadata": {
    "papermill": {
     "duration": 0.01753,
     "end_time": "2025-11-13T06:19:44.955766",
     "exception": false,
     "start_time": "2025-11-13T06:19:44.938236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2d8718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:44.992721Z",
     "iopub.status.busy": "2025-11-13T06:19:44.992021Z",
     "iopub.status.idle": "2025-11-13T06:19:44.997686Z",
     "shell.execute_reply": "2025-11-13T06:19:44.997038Z"
    },
    "papermill": {
     "duration": 0.025246,
     "end_time": "2025-11-13T06:19:44.998747",
     "exception": false,
     "start_time": "2025-11-13T06:19:44.973501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_and_optimizer(model_architecture, optimizer, path_hist,\n",
    "                             base_dir=\"/kaggle/input/mod5/pytorch/default/1\"):\n",
    "    \"\"\"\n",
    "    Memuat model + optimizer dari checkpoint terakhir yang tercatat di CSV.\n",
    "\n",
    "    Args:\n",
    "        model_architecture (nn.Module): arsitektur model yang SAMA.\n",
    "        optimizer (torch.optim.Optimizer): optimizer yang sudah dibuat (Adam, dll).\n",
    "        path_hist (str): path CSV yang berisi kolom 'nama_model'.\n",
    "        base_dir (str): folder tempat file .pth disimpan.\n",
    "\n",
    "    Returns:\n",
    "        model_architecture, optimizer, last_epoch\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # baca CSV history\n",
    "    df = pd.read_csv(path_hist)\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"CSV history kosong, tidak ada checkpoint untuk di-load.\")\n",
    "\n",
    "    last_row = df.iloc[-1]\n",
    "    ckpt_name = last_row['nama_model']\n",
    "    ckpt_path = os.path.join(base_dir, ckpt_name)\n",
    "\n",
    "    print(f\"Load checkpoint dari: {ckpt_path}\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    # load state model & optimizer\n",
    "    model_architecture.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optim_state'])\n",
    "\n",
    "    model_architecture.to(device)\n",
    "    model_architecture.train()   # karena mau lanjut training\n",
    "\n",
    "    last_epoch = int(checkpoint.get('epoch', 0))\n",
    "    print(f\"Berhasil load. last_epoch_global = {last_epoch}\")\n",
    "\n",
    "    return model_architecture, optimizer, last_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc2f3398",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:45.035171Z",
     "iopub.status.busy": "2025-11-13T06:19:45.034736Z",
     "iopub.status.idle": "2025-11-13T06:19:45.078922Z",
     "shell.execute_reply": "2025-11-13T06:19:45.078376Z"
    },
    "papermill": {
     "duration": 0.063838,
     "end_time": "2025-11-13T06:19:45.080070",
     "exception": false,
     "start_time": "2025-11-13T06:19:45.016232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def pilih_file_weight():\n",
    "    \"\"\"\n",
    "    Membuka file explorer untuk memilih file .pth.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # sembunyikan jendela utama Tkinter\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Pilih file weight (.pth)\",\n",
    "        filetypes=[(\"PyTorch model\", \"*.pth\"), (\"Semua file\", \"*.*\")]\n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# ==== Contoh penggunaan ====\n",
    "# Misalnya kamu sudah punya arsitektur model\n",
    "# model = Cnn_interpolation(k_feature=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79445eb0",
   "metadata": {
    "papermill": {
     "duration": 0.017912,
     "end_time": "2025-11-13T06:19:45.116051",
     "exception": false,
     "start_time": "2025-11-13T06:19:45.098139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e3334de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:45.153889Z",
     "iopub.status.busy": "2025-11-13T06:19:45.153282Z",
     "iopub.status.idle": "2025-11-13T06:19:45.368435Z",
     "shell.execute_reply": "2025-11-13T06:19:45.367754Z"
    },
    "papermill": {
     "duration": 0.235461,
     "end_time": "2025-11-13T06:19:45.369929",
     "exception": false,
     "start_time": "2025-11-13T06:19:45.134468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "PATH = kagglehub.dataset_download(\"mcparadip/anime-faces-waifu2x\")\n",
    "\n",
    "# # print(\"Path to dataset files:\", path)\n",
    "# PATH = \"/home/kanza/Dokumen/file/D2L/transformer/anim/archive\" # <-- GANTI INI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa41c2",
   "metadata": {
    "papermill": {
     "duration": 0.020293,
     "end_time": "2025-11-13T06:19:45.411484",
     "exception": false,
     "start_time": "2025-11-13T06:19:45.391191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db44b0",
   "metadata": {
    "papermill": {
     "duration": 0.020511,
     "end_time": "2025-11-13T06:19:45.452890",
     "exception": false,
     "start_time": "2025-11-13T06:19:45.432379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "073c76c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:45.496055Z",
     "iopub.status.busy": "2025-11-13T06:19:45.495769Z",
     "iopub.status.idle": "2025-11-13T06:19:45.500174Z",
     "shell.execute_reply": "2025-11-13T06:19:45.499483Z"
    },
    "papermill": {
     "duration": 0.027877,
     "end_time": "2025-11-13T06:19:45.501616",
     "exception": false,
     "start_time": "2025-11-13T06:19:45.473739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "# # Tentukan path ke folder gambar Anda\n",
    "# PATH_KE_FOLDER_GAMBAR = PATH # <-- GANTI INI\n",
    "# BATCH_SIZE = 1\n",
    "# NUM_EPOCHS = 1\n",
    "# K_FEATURE = 96\n",
    "\n",
    "# # --- MODIFIKASI DIMULAI DI SINI ---\n",
    "\n",
    "# # 1. Buat dataset Lengkap\n",
    "# full_dataset = SuperResolutionDataset(image_folder=PATH_KE_FOLDER_GAMBAR)\n",
    "# print(f\"Ukuran dataset penuh: {len(full_dataset)}\")\n",
    "\n",
    "# # 2. Tentukan jumlah gambar yang ingin digunakan (misal, 100)\n",
    "# num_images_to_use = 2\n",
    "# indices = list(range(num_images_to_use))\n",
    "\n",
    "# # 3. Buat SUBSET dari dataset penuh\n",
    "# subset_dataset = Subset(full_dataset, indices)\n",
    "# print(f\"Ukuran subset yang akan digunakan: {len(subset_dataset)}\")\n",
    "\n",
    "# # 4. Tentukan ukuran split (sekarang berdasarkan subset_dataset)\n",
    "# train_size = int(0.8 * len(subset_dataset)) # Ini akan menjadi 80\n",
    "# val_size = len(subset_dataset) - train_size   # Ini akan menjadi 20\n",
    "\n",
    "# # 5. Lakukan split pada SUBSET tersebut\n",
    "# train_dataset, val_dataset = random_split(subset_dataset, [train_size, val_size])\n",
    "\n",
    "# print(f\"Ukuran data training: {len(train_dataset)}\")\n",
    "# print(f\"Ukuran data validasi: {len(val_dataset)}\")\n",
    "\n",
    "# # --- MODIFIKASI SELESAI ---\n",
    "\n",
    "# # 6. Buat data loader train dan val (kode ini tetap sama)\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=True, \n",
    "#     num_workers=4,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False, # Tidak perlu di-shuffle untuk validasi\n",
    "#     num_workers=4,\n",
    "#     pin_memory=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8652163",
   "metadata": {
    "papermill": {
     "duration": 0.021556,
     "end_time": "2025-11-13T06:19:45.545097",
     "exception": false,
     "start_time": "2025-11-13T06:19:45.523541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b26b550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:45.588994Z",
     "iopub.status.busy": "2025-11-13T06:19:45.588685Z",
     "iopub.status.idle": "2025-11-13T06:19:45.786008Z",
     "shell.execute_reply": "2025-11-13T06:19:45.785002Z"
    },
    "papermill": {
     "duration": 0.220736,
     "end_time": "2025-11-13T06:19:45.787297",
     "exception": false,
     "start_time": "2025-11-13T06:19:45.566561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21551\n",
      "Ukuran data training: 17240\n",
      "Ukuran data validasi: 4311\n"
     ]
    }
   ],
   "source": [
    "# Tentukan path ke folder gambar Anda\n",
    "PATH_KE_FOLDER_GAMBAR = PATH # <-- GANTI INI\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "K_FEATURE = 96\n",
    "\n",
    "# 1. Buat dataset Lengkap\n",
    "full_dataset =  SuperResolutionDataset(image_folder=PATH_KE_FOLDER_GAMBAR)\n",
    "print(len(full_dataset))\n",
    "\n",
    "# 2. Tentukan ukuran split \n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "# 3.Lakukan split\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Ukuran data training: {len(train_dataset)}\")\n",
    "print(f\"Ukuran data validasi: {len(val_dataset)}\")\n",
    "\n",
    "# 4. buat data loader train dan val\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # Tidak perlu di-shuffle untuk validasi\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f12c3",
   "metadata": {
    "papermill": {
     "duration": 0.018025,
     "end_time": "2025-11-13T06:19:45.823943",
     "exception": false,
     "start_time": "2025-11-13T06:19:45.805918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03e1b0d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:19:45.863207Z",
     "iopub.status.busy": "2025-11-13T06:19:45.862561Z",
     "iopub.status.idle": "2025-11-13T15:53:12.928252Z",
     "shell.execute_reply": "2025-11-13T15:53:12.927492Z"
    },
    "papermill": {
     "duration": 34407.086825,
     "end_time": "2025-11-13T15:53:12.929462",
     "exception": false,
     "start_time": "2025-11-13T06:19:45.842637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:02<00:00, 218MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/vgg.pth\n",
      "Mulai Training di cuda | run_id=2025-11-13_06-19-50 | start_epoch_global=0\n",
      "CSV tulis: /kaggle/working/histor.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 [Train]: 100%|██████████| 17240/17240 [1:15:18<00:00,  3.82it/s]\n",
      "Epoch 1/7 [Val]: 100%|██████████| 4311/4311 [06:40<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] e1 | val_loss=0.050965 | PSNR=22.9499 | SSIM=0.6722 | LPIPS=0.2972\n",
      "[SAVE] model_checkpoints/model_2025-11-13_06-19-50_e1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 [Train]: 100%|██████████| 17240/17240 [1:15:12<00:00,  3.82it/s]\n",
      "Epoch 2/7 [Val]: 100%|██████████| 4311/4311 [06:41<00:00, 10.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] e2 | val_loss=0.091136 | PSNR=18.1850 | SSIM=0.4293 | LPIPS=0.4526\n",
      "[SAVE] model_checkpoints/model_2025-11-13_06-19-50_e2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 [Train]: 100%|██████████| 17240/17240 [1:15:08<00:00,  3.82it/s]\n",
      "Epoch 3/7 [Val]: 100%|██████████| 4311/4311 [06:40<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] e3 | val_loss=0.041647 | PSNR=25.5680 | SSIM=0.8133 | LPIPS=0.3258\n",
      "[SAVE] model_checkpoints/model_2025-11-13_06-19-50_e3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 [Train]: 100%|██████████| 17240/17240 [1:15:08<00:00,  3.82it/s]\n",
      "Epoch 4/7 [Val]: 100%|██████████| 4311/4311 [06:41<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] e4 | val_loss=0.050119 | PSNR=23.5875 | SSIM=0.6242 | LPIPS=0.3185\n",
      "[SAVE] model_checkpoints/model_2025-11-13_06-19-50_e4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 [Train]: 100%|██████████| 17240/17240 [1:15:22<00:00,  3.81it/s]\n",
      "Epoch 5/7 [Val]: 100%|██████████| 4311/4311 [06:42<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] e5 | val_loss=0.027100 | PSNR=28.3338 | SSIM=0.9106 | LPIPS=0.1316\n",
      "[SAVE] model_checkpoints/model_2025-11-13_06-19-50_e5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 [Train]: 100%|██████████| 17240/17240 [1:15:15<00:00,  3.82it/s]\n",
      "Epoch 6/7 [Val]: 100%|██████████| 4311/4311 [06:40<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] e6 | val_loss=0.021803 | PSNR=28.9890 | SSIM=0.9175 | LPIPS=0.1633\n",
      "[SAVE] model_checkpoints/model_2025-11-13_06-19-50_e6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 [Train]: 100%|██████████| 17240/17240 [1:15:06<00:00,  3.83it/s]\n",
      "Epoch 7/7 [Val]: 100%|██████████| 4311/4311 [06:40<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] e7 | val_loss=0.259272 | PSNR=10.3974 | SSIM=0.3973 | LPIPS=0.7067\n",
      "[SAVE] model_checkpoints/model_2025-11-13_06-19-50_e7.pth\n",
      "Training Selesai.\n",
      "\n",
      "--- Hasil History Training ---\n",
      "{\n",
      "  \"train_loss\": [\n",
      "    1.497183459562848,\n",
      "    0.30739055953167915,\n",
      "    0.17685998165423564,\n",
      "    0.09085268963051618,\n",
      "    0.07927366539794334,\n",
      "    0.050196612394401874,\n",
      "    0.23670214630205297\n",
      "  ],\n",
      "  \"val_loss\": [\n",
      "    0.05096535526459717,\n",
      "    0.09113559377971865,\n",
      "    0.041646665010132054,\n",
      "    0.05011907097525122,\n",
      "    0.027100421650310565,\n",
      "    0.021803079319871986,\n",
      "    0.25927160166305946\n",
      "  ],\n",
      "  \"val_psnr\": [\n",
      "    22.949886322021484,\n",
      "    18.18497657775879,\n",
      "    25.567951202392578,\n",
      "    23.587453842163086,\n",
      "    28.333757400512695,\n",
      "    28.98896026611328,\n",
      "    10.397409439086914\n",
      "  ],\n",
      "  \"val_ssim\": [\n",
      "    0.6721870303153992,\n",
      "    0.429267942905426,\n",
      "    0.8133188486099243,\n",
      "    0.6242305636405945,\n",
      "    0.9106038212776184,\n",
      "    0.9174782633781433,\n",
      "    0.3973488509654999\n",
      "  ],\n",
      "  \"val_lpips\": [\n",
      "    0.2972051712123293,\n",
      "    0.4525863854964187,\n",
      "    0.3258387049730514,\n",
      "    0.31849386732626944,\n",
      "    0.131565828648867,\n",
      "    0.1632676784115915,\n",
      "    0.7066578105717684\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 7\n",
    "# ================================================== inisialisasi model ==========================================\n",
    "model = RIR_interpolation(\n",
    "    N=1,\n",
    "    C=3,\n",
    "    H=128,\n",
    "    W=128,\n",
    "    k_feature=96,\n",
    "    j_group=6,\n",
    "    j_blocks=6)\n",
    "\n",
    "model = model.to(device)           # ⬅️ pindahkan SEMUA parameter model ke GPU\n",
    "\n",
    "# tentukan loss function\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# menentukan optimizer \n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "\n",
    "# ================================= Trainig ===================================================================\n",
    "# 6. Mulai Training dan TANGKAP output history\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,  # Berikan val_loader di sini\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    save_dir=\"model_checkpoints\",\n",
    "    continue_global_epoch=False\n",
    ")\n",
    "\n",
    "# 7. Tampilkan hasil history\n",
    "print(\"\\n--- Hasil History Training ---\")\n",
    "import json\n",
    "print(json.dumps(history, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9549019",
   "metadata": {
    "papermill": {
     "duration": 5.846068,
     "end_time": "2025-11-13T15:53:24.786018",
     "exception": false,
     "start_time": "2025-11-13T15:53:18.939950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "653acc46",
   "metadata": {
    "papermill": {
     "duration": 6.022801,
     "end_time": "2025-11-13T15:53:36.462822",
     "exception": false,
     "start_time": "2025-11-13T15:53:30.440021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a4fe51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:53:48.040644Z",
     "iopub.status.busy": "2025-11-13T15:53:48.039829Z",
     "iopub.status.idle": "2025-11-13T15:53:48.044484Z",
     "shell.execute_reply": "2025-11-13T15:53:48.043913Z"
    },
    "papermill": {
     "duration": 5.703887,
     "end_time": "2025-11-13T15:53:48.045532",
     "exception": false,
     "start_time": "2025-11-13T15:53:42.341645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 2\n",
    "\n",
    "# # 1. Bangun arsitektur model\n",
    "# model = RIR_interpolation(\n",
    "#     N=1,\n",
    "#     C=3,\n",
    "#     H=128,\n",
    "#     W=128,\n",
    "#     k_feature=96,\n",
    "#     j_group=6,\n",
    "#     j_blocks=6\n",
    "# )\n",
    "\n",
    "# # 2. Buat optimizer (wajib sebelum load optim_state)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# # 3. Load model + optimizer dari checkpoint terakhir\n",
    "# model, optimizer, last_epoch = load_model_and_optimizer(\n",
    "#     model_architecture=model,\n",
    "#     optimizer=optimizer,\n",
    "#     path_hist=\"/kaggle/input/hist123/histor.csv\",\n",
    "#     base_dir=\"/kaggle/input/epoch-optim/pytorch/default/1\"\n",
    "# )\n",
    "\n",
    "# criterion = nn.L1Loss()\n",
    "\n",
    "# # 4. Lanjut training\n",
    "# history = train_model(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     criterion=criterion,\n",
    "#     optimizer=optimizer,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     save_dir=\"/kaggle/working/model_checkpoints\",\n",
    "#     seed_csv=\"/kaggle/input/history2/histor.csv\",   # CSV lama → di-copy ke working\n",
    "#     path_write=\"/kaggle/working/histor.csv\",\n",
    "#     continue_global_epoch=True                      # lanjut nomor epoch\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e307e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:53:59.917672Z",
     "iopub.status.busy": "2025-11-13T15:53:59.917004Z",
     "iopub.status.idle": "2025-11-13T15:53:59.920126Z",
     "shell.execute_reply": "2025-11-13T15:53:59.919599Z"
    },
    "papermill": {
     "duration": 5.92478,
     "end_time": "2025-11-13T15:53:59.921124",
     "exception": false,
     "start_time": "2025-11-13T15:53:53.996344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('/kaggle/working/histor.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f5164",
   "metadata": {
    "papermill": {
     "duration": 5.748189,
     "end_time": "2025-11-13T15:54:11.298980",
     "exception": false,
     "start_time": "2025-11-13T15:54:05.550791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 671155,
     "sourceId": 1181323,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34560.508747,
   "end_time": "2025-11-13T15:54:20.016852",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-13T06:18:19.508105",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
